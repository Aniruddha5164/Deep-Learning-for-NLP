{
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q57Zf8p9ivZa",
        "J5d58CfLoWZj",
        "zdcR5JHdp6qY",
        "NBtXo_Tgw43o"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b467083e9b8e48a28fb390e08d7f2f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3504bd3fded42feac88374368a90820",
              "IPY_MODEL_5d7e45d7faa74b559b8e7abd68fef7ab",
              "IPY_MODEL_9fe8ebab28d0456d8345b6f0dbdef03a"
            ],
            "layout": "IPY_MODEL_be0f177a0db04aeb988c22588c3e95ef"
          }
        },
        "e3504bd3fded42feac88374368a90820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04362094140f43cdac4bfb163aeca3b5",
            "placeholder": "​",
            "style": "IPY_MODEL_7a4b8ea2a13443dc8775ab76b9785716",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5d7e45d7faa74b559b8e7abd68fef7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe372d22b3ab46138e676da79a593da3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ba45990a8124412b8bae4d5dca59130",
            "value": 2
          }
        },
        "9fe8ebab28d0456d8345b6f0dbdef03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91251c4ec6564aa7ab1b3536c5165840",
            "placeholder": "​",
            "style": "IPY_MODEL_72e00893957e4aa6a9aa33469d8b7f29",
            "value": " 2/2 [00:01&lt;00:00,  1.71s/it]"
          }
        },
        "be0f177a0db04aeb988c22588c3e95ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04362094140f43cdac4bfb163aeca3b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4b8ea2a13443dc8775ab76b9785716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe372d22b3ab46138e676da79a593da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba45990a8124412b8bae4d5dca59130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91251c4ec6564aa7ab1b3536c5165840": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72e00893957e4aa6a9aa33469d8b7f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning for NLP**\n",
        "**-Aniruddha Banerjee**"
      ],
      "metadata": {
        "id": "uoNfaQOVoWQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Objectives:**\n",
        "Industry Selection: Students must select one industry from the list provided. This industry will be the focus of their project, including data collection and model training.\n",
        "\n",
        "\n",
        "Data Collection: Gather relevant data specific to the chosen industry. This data will be used to fine-tune the pre-trained model to ensure the LLM Bot is knowledgeable and contextually aware of industry-specific information.\n",
        "\n",
        "\n",
        "\n",
        "Model Selection and Training: Utilize any pre-trained model from Hugging Face or similar platforms. Fine-tune the model on the collected data using resources like Google Colab with T4 GPUs, limiting the training to a maximum of 25 epochs to ensure feasibility.\n",
        "\n",
        "\n",
        "\n",
        "Bot Development: Develop the LLM Bot that can interact with users, providing answers and engaging in meaningful conversations specific to the chosen industry. The bot should demonstrate the ability to understand and process industry-related queries effectively.\n",
        "\n",
        "\n",
        "\n",
        "Demonstration: Create an explanatory video showcasing the working of the LLM Bot. The video should highlight the bot's ability to handle industry-specific questions, demonstrating its practical application."
      ],
      "metadata": {
        "id": "11YKFYleob9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#** Installing the Required Packages  and Libraries**"
      ],
      "metadata": {
        "id": "q57Zf8p9ivZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "BNrsPceayp4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59801e7f-1833-47c5-870f-a39c738dc7e7",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:47:01.025595Z",
          "iopub.execute_input": "2024-07-08T07:47:01.025964Z",
          "iopub.status.idle": "2024-07-08T07:47:29.548179Z",
          "shell.execute_reply.started": "2024-07-08T07:47:01.025934Z",
          "shell.execute_reply": "2024-07-08T07:47:29.546892Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This package installation command includes several different packages that are related to machine learning and natural language processing. Here are the packages listed in the command:\n",
        "\n",
        "accelerate (version 0.21.0): This library is part of the Hugging Face ecosystem and is designed to streamline the process of training and deploying machine learning models on various hardware accelerators (e.g., GPUs, TPUs).\n",
        "\n",
        "peft (version 0.4.0): PEFT stands for \"Parameter-Efficient Fine-Tuning.\" It's used for fine-tuning pre-trained models with fewer parameters, making the process more efficient.\n",
        "\n",
        "bitsandbytes (version 0.40.2): This library is used for 8-bit optimizers and quantization of neural networks, which can significantly reduce memory usage and computational costs during training and inference.\n",
        "\n",
        "transformers (version 4.31.0): This is the main library from Hugging Face that provides implementations of state-of-the-art transformer models for natural language understanding and generation tasks.\n",
        "\n",
        "trl (version 0.4.7): TRL stands for \"Training Reinforcement Learning.\" This library provides tools for training models using reinforcement learning techniques, particularly in the context of natural language processing.\n",
        "\n",
        "Together, these packages facilitate the development, training, fine-tuning, and deployment of advanced machine learning models, particularly those based on transformer architectures."
      ],
      "metadata": {
        "id": "iYznUU9HpP9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "bPez_ItJGWOa",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:47:29.551263Z",
          "iopub.execute_input": "2024-07-08T07:47:29.551548Z",
          "iopub.status.idle": "2024-07-08T07:47:42.023495Z",
          "shell.execute_reply.started": "2024-07-08T07:47:29.551524Z",
          "shell.execute_reply": "2024-07-08T07:47:42.022550Z"
        },
        "trusted": true,
        "outputId": "8074d005-dc0f-4f9a-a38d-8cbf251c126c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command !pip install huggingface_hub installs the Hugging Face Hub library, which is a crucial tool for interacting with the Hugging Face model and dataset repositories."
      ],
      "metadata": {
        "id": "N1cTABuUpXiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "jw-5oFU3ysk9",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:47:42.024785Z",
          "iopub.execute_input": "2024-07-08T07:47:42.025061Z",
          "iopub.status.idle": "2024-07-08T07:48:01.873411Z",
          "shell.execute_reply.started": "2024-07-08T07:47:42.025037Z",
          "shell.execute_reply": "2024-07-08T07:48:01.872639Z"
        },
        "trusted": true,
        "outputId": "eb36e7df-4f3a-4a23-fe23-8fcc3c39d85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-07-08 07:47:49.926511: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-08 07:47:49.926646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-08 07:47:50.061199: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided import statements are bringing in various tools and libraries that are essential for machine learning, specifically for working with natural language processing (NLP) models. Here's a brief explanation of each import and its purpose:\n",
        "\n",
        "Standard Library\n",
        "os: Provides a way to interact with the operating system, including file and directory manipulation.\n",
        "\n",
        "\n",
        "PyTorch\n",
        "torch: The main library for tensor operations and building neural networks in PyTorch.\n",
        "\n",
        "\n",
        "Hugging Face Datasets\n",
        "load_dataset: A function from the datasets library that allows you to load various datasets directly from the Hugging Face Hub\n",
        "\n",
        "\n",
        "\n",
        "Hugging Face Transformers\n",
        "AutoModelForCausalLM: A class that automatically loads a pre-trained causal language model for tasks like text generation.\n",
        "\n",
        "\n",
        "AutoTokenizer: A class that automatically loads the appropriate tokenizer for a given model.\n",
        "\n",
        "\n",
        "BitsAndBytesConfig: A configuration class for quantization and 8-bit optimizers from the bitsandbytes library.\n",
        "\n",
        "\n",
        "HfArgumentParser: A parser for command-line arguments that integrates well with Hugging Face's training scripts.\n",
        "\n",
        "\n",
        "TrainingArguments: A class that stores all the arguments needed to train a model, including learning rate, batch size, and more.\n",
        "\n",
        "\n",
        "pipeline: A high-level API for running various NLP tasks (e.g., text generation, text classification) using pre-trained models.\n",
        "\n",
        "\n",
        "logging: A module for setting up logging for the transformers library.\n",
        "Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "\n",
        "LoraConfig: A configuration class for LoRA (Low-Rank Adaptation), which is a technique for parameter-efficient fine-tuning.\n",
        "\n",
        "\n",
        "PeftModel: A class that wraps a model with PEFT methods, such as LoRA.\n",
        "Training with Reinforcement Learning\n",
        "\n",
        "\n",
        "SFTTrainer: A trainer class from the trl library for training models using reinforcement learning techniques, specifically for NLP tasks.\n",
        "\n",
        "\n",
        "These imports collectively allow for building, training, and fine-tuning state-of-the-art NLP models efficiently, leveraging advanced techniques such as parameter-efficient fine-tuning and model quantization."
      ],
      "metadata": {
        "id": "UF92q3GqpdW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:01.875808Z",
          "iopub.execute_input": "2024-07-08T07:48:01.876270Z",
          "iopub.status.idle": "2024-07-08T07:48:01.883024Z",
          "shell.execute_reply.started": "2024-07-08T07:48:01.876238Z",
          "shell.execute_reply": "2024-07-08T07:48:01.882186Z"
        },
        "trusted": true,
        "id": "sWb2YqApn-VT",
        "outputId": "9c200482-0cc5-47b0-9552-f8846030094a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading and Preparing Datasets for NLP Model Training**\n",
        "The following code snippet demonstrates how to load datasets from the Hugging Face Hub, which are then used for training a natural language processing (NLP) model. Specifically, it loads two datasets related to financial texts and prepares them for training."
      ],
      "metadata": {
        "id": "J5d58CfLoWZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset1 = load_dataset(\"poornima9348/finance-alpaca-1k-test\")\n",
        "dataset2 = load_dataset(\"ssbuild/alpaca_finance_en\")"
      ],
      "metadata": {
        "id": "_4_4BwpuiJEH",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:01.884072Z",
          "iopub.execute_input": "2024-07-08T07:48:01.884337Z",
          "iopub.status.idle": "2024-07-08T07:48:06.144535Z",
          "shell.execute_reply.started": "2024-07-08T07:48:01.884315Z",
          "shell.execute_reply": "2024-07-08T07:48:06.143781Z"
        },
        "trusted": true,
        "outputId": "066c35e6-2978-4c56-9e1a-d6399d600a42",
        "colab": {
          "referenced_widgets": [
            "0d5c1c45783a4af5b1a0252237906230",
            "4a78d6a2a0f446fc99dc4931605ce153",
            "80ef02c119944baa90fb6d1830fc17f3",
            "ef230220a546429c8db3a50b673a02dd",
            "3e4cff120c064117ac05cd0073aa0fe6",
            "24df77b3eb314c4db6010e8336ac8546"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/121 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d5c1c45783a4af5b1a0252237906230"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/1.11M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a78d6a2a0f446fc99dc4931605ce153"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80ef02c119944baa90fb6d1830fc17f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef230220a546429c8db3a50b673a02dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/23.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4cff120c064117ac05cd0073aa0fe6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/68912 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24df77b3eb314c4db6010e8336ac8546"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1"
      ],
      "metadata": {
        "id": "zno1pBMwiMLn",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:06.145592Z",
          "iopub.execute_input": "2024-07-08T07:48:06.145891Z",
          "iopub.status.idle": "2024-07-08T07:48:06.151619Z",
          "shell.execute_reply.started": "2024-07-08T07:48:06.145864Z",
          "shell.execute_reply": "2024-07-08T07:48:06.150730Z"
        },
        "trusted": true,
        "outputId": "1b121e1e-42e3-4b7b-d952-bc233a40dec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 1000\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2"
      ],
      "metadata": {
        "id": "twCHMEZtiPZQ",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:06.152744Z",
          "iopub.execute_input": "2024-07-08T07:48:06.153039Z",
          "iopub.status.idle": "2024-07-08T07:48:06.243255Z",
          "shell.execute_reply.started": "2024-07-08T07:48:06.153016Z",
          "shell.execute_reply": "2024-07-08T07:48:06.242379Z"
        },
        "trusted": true,
        "outputId": "2c854a35-285a-4d60-d2f3-e031eadd8578"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'instruction', 'input', 'output'],\n        num_rows: 68912\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code snippet and dataset details indicate the structure and content of dataset1 and dataset2"
      ],
      "metadata": {
        "id": "XgP7s6X5p-vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transforming and Cleaning the Datasets for Model Training**\n",
        "In this code snippet, we are transforming and cleaning the datasets to prepare them for training a language model. Specifically, we combine the instruction and output columns into a new text column and then remove unnecessary columns from the datasets."
      ],
      "metadata": {
        "id": "GqiPimgEqBjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine 'instruction' and 'output' columns into a new 'text' column\n",
        "def combine_text_columns(example):\n",
        "    return {'text': f\"{example['instruction']} ### {example['output']}\"}\n",
        "\n",
        "# Apply the function to each example in the dataset\n",
        "dataset1 = dataset1.map(combine_text_columns)\n",
        "dataset2 = dataset2.map(combine_text_columns)\n",
        "\n",
        "# Remove 'instruction', 'input' and 'output' columns\n",
        "dataset1['test']=dataset1['test'].remove_columns(['instruction','input', 'output'])\n",
        "dataset2['train']=dataset2['train'].remove_columns(['instruction','input', 'output','id'])"
      ],
      "metadata": {
        "id": "_ecqAmGv7Bjm",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:06.244568Z",
          "iopub.execute_input": "2024-07-08T07:48:06.244902Z",
          "iopub.status.idle": "2024-07-08T07:48:12.404242Z",
          "shell.execute_reply.started": "2024-07-08T07:48:06.244873Z",
          "shell.execute_reply": "2024-07-08T07:48:12.403369Z"
        },
        "trusted": true,
        "outputId": "fc4ad33f-31d0-4c2b-84f7-14b357424d26",
        "colab": {
          "referenced_widgets": [
            "9e07119cbf9544d798f43b3be274a3f4",
            "033c965d05ea49d8bf246afaa8b8215f"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e07119cbf9544d798f43b3be274a3f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/68912 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "033c965d05ea49d8bf246afaa8b8215f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Train-Test Split on the Datasets**\n",
        "In this section, we perform a train-test split on the datasets to prepare separate training and testing subsets. This is a crucial step in machine learning to evaluate the performance of the model on unseen data."
      ],
      "metadata": {
        "id": "Ni88OmokqkHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the train-test split on the necessary dataset if required\n",
        "split_dataset1 = dataset1['test'].train_test_split(train_size=0.8)\n",
        "split_dataset2 = dataset2['train'].train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "8Ceyq0vihS6I",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.405548Z",
          "iopub.execute_input": "2024-07-08T07:48:12.405908Z",
          "iopub.status.idle": "2024-07-08T07:48:12.591503Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.405876Z",
          "shell.execute_reply": "2024-07-08T07:48:12.590538Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset1"
      ],
      "metadata": {
        "id": "h0g-eEQhhcS1",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.594356Z",
          "iopub.execute_input": "2024-07-08T07:48:12.594640Z",
          "iopub.status.idle": "2024-07-08T07:48:12.676255Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.594616Z",
          "shell.execute_reply": "2024-07-08T07:48:12.675290Z"
        },
        "trusted": true,
        "outputId": "e822db08-0932-4fd2-9bb0-2030d3e2e66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 800\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 200\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset2"
      ],
      "metadata": {
        "id": "b-CJCK3IikOm",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.677465Z",
          "iopub.execute_input": "2024-07-08T07:48:12.677852Z",
          "iopub.status.idle": "2024-07-08T07:48:12.685802Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.677803Z",
          "shell.execute_reply": "2024-07-08T07:48:12.684780Z"
        },
        "trusted": true,
        "outputId": "75db501e-1c46-4c26-972f-6bcfc9576c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55129\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13783\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Concatenating and Merging Datasets for Training**\n",
        "In this section, we concatenate the training and testing splits from the two datasets to create a single, unified dataset. This step combines the data, ensuring that the model is trained and evaluated on a larger and more diverse dataset."
      ],
      "metadata": {
        "id": "RLQIk9D8qnfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the datasets\n",
        "merged_train = concatenate_datasets([split_dataset1['train'], split_dataset2['train']])\n",
        "merged_test = concatenate_datasets([split_dataset1['test'], split_dataset2['test']])\n",
        "\n",
        "# Create a new DatasetDict with the merged datasets\n",
        "merged_dataset = DatasetDict({\n",
        "    'train': merged_train,\n",
        "    'test': merged_test\n",
        "})\n",
        "\n",
        "# Filter out None values in case some splits are missing\n",
        "merged_dataset = DatasetDict({k: v for k, v in merged_dataset.items() if v is not None})\n",
        "\n",
        "# Print the merged dataset to verify\n",
        "print(merged_dataset)"
      ],
      "metadata": {
        "id": "Azyahwk_ir69",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.687038Z",
          "iopub.execute_input": "2024-07-08T07:48:12.687786Z",
          "iopub.status.idle": "2024-07-08T07:48:12.702890Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.687760Z",
          "shell.execute_reply": "2024-07-08T07:48:12.701884Z"
        },
        "trusted": true,
        "outputId": "d6286337-2047-4d0b-95bc-84c9d901e49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 55929\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 13983\n    })\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Shuffling, Slicing, and Transforming the Dataset**\n",
        "In this section, we shuffle and slice the training dataset, then transform the conversation text into a new format suitable for training an NLP model."
      ],
      "metadata": {
        "id": "0RSWiMfYqzPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and slice it\n",
        "merged_train_dataset = merged_dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "\n",
        "def transform_conversation(example):\n",
        "    conversation_text1 = example['text']\n",
        "    segments = conversation_text1.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over the segments and ensure each segment has a prompt and answer\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        if i + 1 < len(segments):\n",
        "            answer = segments[i + 1].strip()\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_dataset = merged_train_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "id": "NT6hchLYdTmS",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:12.703904Z",
          "iopub.execute_input": "2024-07-08T07:48:12.704187Z",
          "iopub.status.idle": "2024-07-08T07:48:13.225899Z",
          "shell.execute_reply.started": "2024-07-08T07:48:12.704165Z",
          "shell.execute_reply": "2024-07-08T07:48:13.224997Z"
        },
        "trusted": true,
        "outputId": "e72554ce-d926-4e52-c851-9fe0685cac10",
        "colab": {
          "referenced_widgets": [
            "fdd808070d3a4197ac02b145b4b81921"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd808070d3a4197ac02b145b4b81921"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " We again shuffle,slice and slice the dataset with a difference.\n",
        " Dataset Size: The training dataset is larger (5000 examples) compared to the test dataset (100 examples).\n",
        "Purpose: Training data is prepared in larger quantities to help the model learn, while test data is smaller and used to evaluate model performance."
      ],
      "metadata": {
        "id": "gkFjg0Myq6xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and slice it\n",
        "merged_test_dataset = merged_dataset['test'].shuffle(seed=42).select(range(100))\n",
        "\n",
        "def transform_conversation(example):\n",
        "    conversation_text1 = example['text']\n",
        "    segments = conversation_text1.split('###')\n",
        "\n",
        "    reformatted_segments = []\n",
        "\n",
        "    # Iterate over the segments and ensure each segment has a prompt and answer\n",
        "    for i in range(0, len(segments) - 1, 2):\n",
        "        prompt = segments[i].strip()\n",
        "        if i + 1 < len(segments):\n",
        "            answer = segments[i + 1].strip()\n",
        "            # Apply the new template\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] {answer} </s>')\n",
        "        else:\n",
        "            # Handle the case where there is no corresponding assistant segment\n",
        "            reformatted_segments.append(f'<s>[INST] {prompt} [/INST] </s>')\n",
        "\n",
        "    return {'text': ''.join(reformatted_segments)}\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_test_dataset = merged_test_dataset.map(transform_conversation)"
      ],
      "metadata": {
        "id": "F6QT7Kv4w7wG",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.227016Z",
          "iopub.execute_input": "2024-07-08T07:48:13.227289Z",
          "iopub.status.idle": "2024-07-08T07:48:13.281438Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.227265Z",
          "shell.execute_reply": "2024-07-08T07:48:13.280585Z"
        },
        "trusted": true,
        "outputId": "f17101e3-fff6-4313-a956-745b29a00f99",
        "colab": {
          "referenced_widgets": [
            "2fcfae05100a49d59a544780110a9b18"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fcfae05100a49d59a544780110a9b18"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_test_dataset['text'][0]"
      ],
      "metadata": {
        "id": "acgKXbKWxMXH",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.282512Z",
          "iopub.execute_input": "2024-07-08T07:48:13.282793Z",
          "iopub.status.idle": "2024-07-08T07:48:13.293796Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.282769Z",
          "shell.execute_reply": "2024-07-08T07:48:13.292887Z"
        },
        "trusted": true,
        "outputId": "c9bc89c9-65a7-497b-ee21-68130c7f664a"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'In USA, what circumstances (if any) make it illegal for a homeless person to “rent” an address? ### It depends on the rules in the specific places you stay.  Specific places being countries or states.   Some states may consider pension payments to be taxable income, others may not.  Some may consider presence for X days to constitute residency, X days may be 60 days in a calendar year whether or not those days are continuous.   It doesn\\'t matter so much where your mailbox or mail handling service is located, it matters: You may owe taxes in more than one place.  Some states will allow you to offset other states\\' taxes against theirs.  Some states in the US are really harsh on income taxes.  It\\'s my understanding that if you own real estate in New York, all of your income, no matter the source, is taxable income in New York whether or not you were ever in the state that year. Ultimately, you can\\'t just put up your hand and say, \"that\\'s my tax domicile so I\\'m exempt from all your taxes.\"  There is no umbrella US regulation on this topic, the states determine who they consider to be residents and how those residents are to be taxed. While it\\'s possible you may be considered a resident of multiple states and owe income taxes in multiple states, it\\'s equally possible that you won\\'t meet the residency criteria for any state regardless of whether or not that state has an income tax.  The issue you face, as addressed in @Jay\\'s answer, Oklahoma will consider you a resident of OK until you have established residency somewhere else.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Configuration for Fine-Tuning a Model**\n",
        "This code sets up the fine-tuning of a pre-trained model from the Hugging Face Hub using specific configurations. It specifies parameters for QLoRA to adapt the model with low-rank adaptations, uses 4-bit precision with bitsandbytes for efficient computation, and defines training arguments such as learning rate, batch size, and gradient accumulation. It also configures sequence handling and checkpointing for the training process. Overall, it prepares and customizes the model training setup for optimal performance and resource management."
      ],
      "metadata": {
        "id": "ZSzQxutXq_tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-finance-chatbot-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = 350\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "MFdhJds4yzuh",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.295083Z",
          "iopub.execute_input": "2024-07-08T07:48:13.295345Z",
          "iopub.status.idle": "2024-07-08T07:48:13.304394Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.295323Z",
          "shell.execute_reply": "2024-07-08T07:48:13.303526Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fine-Tuning Configuration for a Pre-Trained Model**"
      ],
      "metadata": {
        "id": "1hoU_AzdrHw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Reformatted dataset\n",
        "dataset = transformed_dataset\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zqiIFRObOjgh",
        "execution": {
          "iopub.status.busy": "2024-07-08T07:48:13.305806Z",
          "iopub.execute_input": "2024-07-08T07:48:13.306167Z",
          "iopub.status.idle": "2024-07-08T08:38:28.648781Z",
          "shell.execute_reply.started": "2024-07-08T07:48:13.306136Z",
          "shell.execute_reply": "2024-07-08T08:38:28.647872Z"
        },
        "trusted": true,
        "outputId": "c2ec951f-dce5-4bca-d6f7-970c64974c60",
        "colab": {
          "referenced_widgets": [
            "cdb4df6ae92148bc90c03ca9c649b4a1",
            "57fc82bd2a5b477eb727296252c64db5",
            "b71f35e7c7184863bd08c173917fc2cf",
            "bf667e14cecd4cc796f5eb79e6b09f3c",
            "84e9c98590a345569ab971a01a0e2830",
            "53e13b5cc8e14791a03804c5f3d43b81",
            "d39b2246aa7343f29ccb5d0e96014aad",
            "e416b097b01f4ee8bbbd32cd69a62616",
            "823dce8aeed14ea08559ab3b90f0a21f",
            "5eacc19acf76440c8e7b5e46b88ca5bf",
            "6d4036225c1040c880d42d47d9cb3211",
            "04cb610c1f6c454cbb19e2632930ee18",
            "02b694ad35ac43a1b6ff6ff30ef0e4e6"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb4df6ae92148bc90c03ca9c649b4a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57fc82bd2a5b477eb727296252c64db5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b71f35e7c7184863bd08c173917fc2cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf667e14cecd4cc796f5eb79e6b09f3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84e9c98590a345569ab971a01a0e2830"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53e13b5cc8e14791a03804c5f3d43b81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d39b2246aa7343f29ccb5d0e96014aad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e416b097b01f4ee8bbbd32cd69a62616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "823dce8aeed14ea08559ab3b90f0a21f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5eacc19acf76440c8e7b5e46b88ca5bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d4036225c1040c880d42d47d9cb3211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04cb610c1f6c454cbb19e2632930ee18"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02b694ad35ac43a1b6ff6ff30ef0e4e6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 48:26, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>2.426200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.010200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.746500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.523300</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.795900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.531000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.810100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.453500</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.782000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.516000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.710700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.460700</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.726000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.408600</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.746100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.472400</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.678000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.422200</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.710600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.434900</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.758000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.404400</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.717600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.432500</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.627700</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=625, training_loss=1.6522029724121094, metrics={'train_runtime': 2917.1263, 'train_samples_per_second': 1.714, 'train_steps_per_second': 0.214, 'total_flos': 1.263688331624448e+16, 'train_loss': 1.6522029724121094, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps in the Code:\n",
        "\n",
        "Model Selection:\n",
        "\n",
        "Sets the pre-trained model (NousResearch/Llama-2-7b-chat-hf) and specifies the name for the fine-tuned model (Llama-2-7b-finance-chatbot-finetune).\n",
        "QLoRA Parameters:\n",
        "\n",
        "Configures Low-Rank Adaptation (LoRA) parameters including attention dimension, scaling factor, and dropout probability.\n",
        "bitsandbytes Parameters:\n",
        "\n",
        "Enables 4-bit precision for efficient computation, with settings for data type, quantization type, and optional nested quantization.\n",
        "TrainingArguments:\n",
        "\n",
        "Defines training parameters such as output directory, number of epochs, batch sizes, learning rate, optimizer, and checkpointing/logging settings.\n",
        "SFT Parameters:\n",
        "\n",
        "Specifies sequence length, packing of short examples, and device mapping for model loading and training"
      ],
      "metadata": {
        "id": "eLY6dERbrUe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Saving and Verifying the Trained Model**"
      ],
      "metadata": {
        "id": "CcRUlg8_ra2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "baYvk8rm92t-",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:42:25.992497Z",
          "iopub.execute_input": "2024-07-08T08:42:25.992906Z",
          "iopub.status.idle": "2024-07-08T08:42:26.288464Z",
          "shell.execute_reply.started": "2024-07-08T08:42:25.992874Z",
          "shell.execute_reply": "2024-07-08T08:42:26.287278Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents to ensure files are saved\n",
        "print(\"Contents of new_model directory:\", os.listdir(new_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIu1IjPpQpxi",
        "outputId": "86477673-5759-47ce-e536-72758f2ae6dc",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:42:47.338471Z",
          "iopub.execute_input": "2024-07-08T08:42:47.338821Z",
          "iopub.status.idle": "2024-07-08T08:42:47.344254Z",
          "shell.execute_reply.started": "2024-07-08T08:42:47.338792Z",
          "shell.execute_reply": "2024-07-08T08:42:47.343309Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Contents of new_model directory: ['adapter_config.json', 'README.md', 'adapter_model.bin']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running Text Generation with the Fine-Tuned Model**"
      ],
      "metadata": {
        "id": "TDqxnjvExGG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt =\"Generate a title for a blog about the Nobel Prize ceremony.'\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=100)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "T8ZhDdD0-LpE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "84e28440-d8fe-4bde-fdb0-00747637f880",
        "execution": {
          "iopub.status.busy": "2024-07-08T03:10:20.432675Z",
          "iopub.execute_input": "2024-07-08T03:10:20.433779Z",
          "iopub.status.idle": "2024-07-08T03:10:41.972639Z",
          "shell.execute_reply.started": "2024-07-08T03:10:20.433733Z",
          "shell.execute_reply": "2024-07-08T03:10:41.971651Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "<s>[INST] Generate a title for a blog about the Nobel Prize ceremony.' [/INST] The Nobel Prize Ceremony: A Celebration of Excellence.\n\nThe Nobel Prize ceremony is an annual event that recognizes the achievements of individuals who have made significant contributions to their respective fields. It is a celebration of excellence and a testament to the power of human ingenuity. The ceremony is a time for the recipients to be recognized for\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps Walkthrough**\n",
        "Ignore Warnings: Set the logging level to show only critical errors.\n",
        "\n",
        "Initialize Text Generation Pipeline: Create a pipeline for text generation using the fine-tuned model and tokenizer, specifying the maximum length of the generated text.\n",
        "\n",
        "Generate Text: Use the pipeline to generate text based on a formatted prompt.\n",
        "\n",
        "Print Generated Text: Output the generated text to the console."
      ],
      "metadata": {
        "id": "NBtXo_Tgw43o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Launch TensorBoard to Visualize Training Logs**"
      ],
      "metadata": {
        "id": "7Cucd4PJrsiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ],
      "metadata": {
        "id": "crj9svNe4hU5",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Reloading, Merging, and Saving**"
      ],
      "metadata": {
        "id": "OpITGTo5rwVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define model_name and new_model\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "new_model = \"Llama-2-7b-finance-chatbot-finetune\"\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(new_model):\n",
        "    os.makedirs(new_model)\n",
        "\n",
        "# Define the offload directory\n",
        "offload_dir = \"/kaggle/working/\"\n",
        "\n",
        "# Ensure the offload directory exists\n",
        "if not os.path.exists(offload_dir):\n",
        "    os.makedirs(offload_dir)\n",
        "\n",
        "try:\n",
        "    # Reload model in FP16 and merge it with LoRA weights\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        low_cpu_mem_usage=True,\n",
        "        return_dict=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",  # Automatically splits the model across available GPUs\n",
        "        offload_folder=offload_dir  # Offload to the specified directory\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, new_model)\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    # Reload tokenizer to save it\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(new_model)\n",
        "    tokenizer.save_pretrained(new_model)\n",
        "\n",
        "    # List the contents to ensure files are saved\n",
        "    print(\"Contents of new_model directory:\", os.listdir(new_model))\n",
        "\n",
        "    # Zip the new_model directory\n",
        "    #shutil.make_archive(new_model, 'zip', new_model)\n",
        "\n",
        "    # Download the zipped file\n",
        "    #files.download(new_model + \".zip\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"Out of memory error. Try using a smaller model or increasing GPU memory.\")\n",
        "        torch.cuda.empty_cache()\n",
        "    else:\n",
        "        raise e\n",
        "except ValueError as e:\n",
        "    print(f\"ValueError: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b467083e9b8e48a28fb390e08d7f2f03",
            "e3504bd3fded42feac88374368a90820",
            "5d7e45d7faa74b559b8e7abd68fef7ab",
            "9fe8ebab28d0456d8345b6f0dbdef03a",
            "be0f177a0db04aeb988c22588c3e95ef",
            "04362094140f43cdac4bfb163aeca3b5",
            "7a4b8ea2a13443dc8775ab76b9785716",
            "fe372d22b3ab46138e676da79a593da3",
            "2ba45990a8124412b8bae4d5dca59130",
            "91251c4ec6564aa7ab1b3536c5165840",
            "72e00893957e4aa6a9aa33469d8b7f29",
            "8bb0b32f5e5f449b863abf20da95ecf1"
          ]
        },
        "id": "EklVbzTfShfX",
        "outputId": "bba316bb-4f1f-4877-c65a-3c3e60b9eb4f",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:43:11.229967Z",
          "iopub.execute_input": "2024-07-08T08:43:11.230787Z",
          "iopub.status.idle": "2024-07-08T08:44:35.506249Z",
          "shell.execute_reply.started": "2024-07-08T08:43:11.230756Z",
          "shell.execute_reply": "2024-07-08T08:44:35.505237Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bb0b32f5e5f449b863abf20da95ecf1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Contents of new_model directory: ['adapter_config.json', 'README.md', 'pytorch_model-00002-of-00002.bin', 'adapter_model.bin', 'tokenizer.json', 'special_tokens_map.json', 'added_tokens.json', 'tokenizer_config.json', 'tokenizer.model', 'pytorch_model.bin.index.json', 'pytorch_model-00001-of-00002.bin', 'config.json', 'generation_config.json']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps in the Code:**\n",
        "\n",
        "Clear GPU Memory:\n",
        "\n",
        "Empty the GPU cache to free up memory.\n",
        "Ensure Directory Exists:\n",
        "\n",
        "Create directories for saving the new model and offloading if they do not already exist.\n",
        "Reload Model:\n",
        "\n",
        "Load the base model in FP16 precision, merge it with LoRA weights, and offload to a specified directory.\n",
        "Reload and Configure Tokenizer:\n",
        "\n",
        "Load the tokenizer, configure padding settings, and save both the model and tokenizer.\n",
        "Verify and Handle Errors:\n",
        "\n",
        "List the directory contents to confirm saving, handle out-of-memory errors, and raise other exceptions if they occur."
      ],
      "metadata": {
        "id": "2bn7tjdByJ_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pushing Model and Tokenizer to Hugging Face Hub"
      ],
      "metadata": {
        "id": "CpyMDvc5yQrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "Z8VUygTdqVnJ",
        "execution": {
          "iopub.status.busy": "2024-07-08T08:52:11.958847Z",
          "iopub.execute_input": "2024-07-08T08:52:11.959238Z",
          "iopub.status.idle": "2024-07-08T08:52:11.963619Z",
          "shell.execute_reply.started": "2024-07-08T08:52:11.959207Z",
          "shell.execute_reply": "2024-07-08T08:52:11.962716Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, whoami\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "import os\n",
        "\n",
        "# Step 1: Retrieve the Hugging Face token from Kaggle Secrets\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"chatbot\")\n",
        "\n",
        "# Step 2: Login using the Hugging Face token\n",
        "login(token=hf_token)\n",
        "\n",
        "\n",
        "# Step 3: Push the model and tokenizer to the Hugging Face Hub\n",
        "model_repo_name = \"anirudh/finance_chatbot\"\n",
        "tokenizer_repo_name = \"anirudh/finance_chatbot\"\n",
        "\n",
        "# Push the model to the hub\n",
        "model.push_to_hub(model_repo_name, use_auth_token=hf_token, check_pr=True)\n",
        "\n",
        "# Push the tokenizer to the hub\n",
        "tokenizer.push_to_hub(tokenizer_repo_name, use_auth_token=hf_token, check_pr=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T08:52:20.916375Z",
          "iopub.execute_input": "2024-07-08T08:52:20.917065Z",
          "iopub.status.idle": "2024-07-08T08:58:01.131950Z",
          "shell.execute_reply.started": "2024-07-08T08:52:20.917032Z",
          "shell.execute_reply": "2024-07-08T08:58:01.131037Z"
        },
        "trusted": true,
        "id": "FJfGIV3fn-Vb",
        "outputId": "06f54564-285f-44ac-cce5-4eff82cf4494",
        "colab": {
          "referenced_widgets": [
            "3fe490fdfbba48619caef236b9236ec5",
            "1f7291e7b2b54fdc9213ea894fb4ca0e",
            "50c46167e6844dd69c92902f54e62ceb"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fe490fdfbba48619caef236b9236ec5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f7291e7b2b54fdc9213ea894fb4ca0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50c46167e6844dd69c92902f54e62ceb"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 24,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/tito92/finance_finetune_model/commit/b3e1572b6b3f267b09be970ef8bfb91300cf812f', commit_message='Upload tokenizer', commit_description='', oid='b3e1572b6b3f267b09be970ef8bfb91300cf812f', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps in the Code:**\n",
        "\n",
        "Set Preferred Encoding:\n",
        "\n",
        "Ensure the preferred encoding is set to \"UTF-8\" to handle text correctly.\n",
        "Retrieve Hugging Face Token:\n",
        "\n",
        "Obtain the Hugging Face authentication token from Kaggle Secrets.\n",
        "Login to Hugging Face:\n",
        "\n",
        "Authenticate with the Hugging Face Hub using the retrieved token.\n",
        "Push Model and Tokenizer to Hub:\n",
        "\n",
        "Upload the fine-tuned model and tokenizer to the Hugging Face Hub under specified repository names."
      ],
      "metadata": {
        "id": "jPZQPSa-sRyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-08T08:58:57.954529Z",
          "iopub.execute_input": "2024-07-08T08:58:57.954906Z",
          "iopub.status.idle": "2024-07-08T08:58:57.987687Z",
          "shell.execute_reply.started": "2024-07-08T08:58:57.954873Z",
          "shell.execute_reply": "2024-07-08T08:58:57.986717Z"
        },
        "trusted": true,
        "id": "5JWnaH1Zn-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Loading Model from Hugging Face Hub and Copying Files**"
      ],
      "metadata": {
        "id": "uBv1zeUIsWrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the finetuned model from Hugging face Hub\n",
        "fine_tuned_finance_model=AutoModelForCausalLM.from_pretrained('anirudh/finance_chatbot')"
      ],
      "metadata": {
        "id": "ofQ-5pnns8_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained('anirudh/finance_chatbot', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "uSgRDWu0n-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source (output) directory and the new target directory\n",
        "source_dir = \"/kaggle/working/output_dir\"\n",
        "target_dir = \"/kaggle/working/new_dir\"\n",
        "\n",
        "# Ensure the target directory exists, create it if it does not\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Specify the files you want to copy (modify this list as needed)\n",
        "files_to_copy = [\"file1.txt\", \"file2.txt\", \"model.pth\", \"config.json\"]\n",
        "\n",
        "# Copy specified files from source directory to target directory\n",
        "for file_name in files_to_copy:\n",
        "    source_file_path = os.path.join(source_dir, file_name)\n",
        "    target_file_path = os.path.join(target_dir, file_name)\n",
        "    if os.path.exists(source_file_path):\n",
        "        shutil.copy2(source_file_path, target_file_path)\n",
        "        print(f\"Copied {file_name} to {target_dir}\")\n",
        "    else:\n",
        "        print(f\"{file_name} not found in {source_dir}\")\n",
        "\n",
        "print(\"File copying complete.\")\n"
      ],
      "metadata": {
        "id": "vGiH09Y-n-Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Steps in the Code:**\n",
        "\n",
        "Load Fine-Tuned Model and Tokenizer:\n",
        "\n",
        "Load the fine-tuned model and tokenizer from the Hugging Face Hub using their respective repository names.\n",
        "Define Source and Target Directories:\n",
        "\n",
        "Specify the source directory (where files are currently located) and the target directory (where files will be copied).\n",
        "Ensure Target Directory Exists:\n",
        "\n",
        "Check if the target directory exists and create it if it does not.\n",
        "Copy Files:\n",
        "\n",
        "Copy specified files from the source directory to the target directory. Log the status of each file copy operation.\n",
        "Complete File Copying:\n",
        "\n",
        "Print a message indicating the completion of the file copying process"
      ],
      "metadata": {
        "id": "AWbTyKm_svrx"
      }
    }
  ]
}